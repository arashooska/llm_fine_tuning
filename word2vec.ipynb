{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bbebd2a-faae-4755-9e5e-a1d2408924ed",
   "metadata": {},
   "source": [
    "### Word2Vec Test\n",
    "Tutorial Source: https://www.tensorflow.org/text/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5707312-1141-453c-a95d-81c918be773d",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d92582-52c0-4291-b9f9-8070224864f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef67579-8a96-44e5-aa7d-93c7ca694cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87316f48-ea64-41e7-a70a-30648c09c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f142c9c-fb68-448d-86f4-4a996ca8c604",
   "metadata": {},
   "source": [
    "#### Vectorize an example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f14791-88aa-4b93-ba97-46501c38af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a6704-c6a3-494d-9d61-311ae8062310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary to save mappings from tokens to integer indices\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e65f2e-9f79-43e1-a233-821775b61955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "# Create an inverse vocabulary to save mappings from integer indices to tokens\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c835bcfc-9076-4190-ae03-e6cecff94452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize your sentence\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b169f0-b976-4c46-a35b-492193bc0e1c",
   "metadata": {},
   "source": [
    "#### Generate skip-grams from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b144a7-862d-4dca-ab7f-81298661f594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      # negative_samples is set to 0 here, we will use another \n",
    "      # function to perform negative sampling later\n",
    "      negative_samples=0) \n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed63080-3110-4d21-a8a5-da53c07cfecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6): (the, hot)\n",
      "(6, 1): (hot, the)\n",
      "(2, 3): (wide, road)\n",
      "(1, 7): (the, sun)\n",
      "(7, 1): (sun, the)\n"
     ]
    }
   ],
   "source": [
    "# Print a few positive skip-grams\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54f47a-0e9d-4e8a-a982-a8fe35144326",
   "metadata": {},
   "source": [
    "#### Negative sampling for one skip-gram\n",
    "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled. <br>\n",
    "`num_ns` (the number of negative samples per a positive context word) in the [5, 20] range is shown to work best for smaller datasets, while num_ns in the [2, 5] range suffices for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf817296-c377-4f0b-a63f-92814e0b180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
      "['wide', 'the', 'shimmered', 'road']\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c920f-9e8a-4d08-ac8f-b9a0ca51c8c2",
   "metadata": {},
   "source": [
    "#### Construct one training example\n",
    "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the 1 positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labeled as `1`) and negative samples (labeled as `0`) for each target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01280790-4e30-40df-b907-20d852782fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce a dimension so you can use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43734b04-2b9e-48b0-9a15-ffe6ad8e6c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1\n",
      "target_word     : the\n",
      "context_indices : [6 2 1 4 3]\n",
      "context_words   : ['hot', 'wide', 'the', 'shimmered', 'road']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Check out the context and the corresponding labels for the target word from the skip-gram example above\n",
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e1257-500e-47e2-9398-98db36065cc2",
   "metadata": {},
   "source": [
    "A tuple of `(target, context, label)` tensors constitutes one training example for training your skip-gram negative sampling word2vec model. Notice that the target is of shape `(1,)` while the context and label are of shape `(1+num_ns,)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19430da-78fb-45d8-97d3-a1a72a7a33f5",
   "metadata": {},
   "source": [
    "#### Compile all steps into one function\n",
    "##### Skip-gram sampling table\n",
    "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as `the`, `is`, `on`) don't add much useful information for the model to learn from. It is suggested that subsampling of frequent words as a helpful practice to improve embedding quality.<br>\n",
    "The tf.keras.preprocessing.sequence.skipgrams function accepts a sampling table argument to encode probabilities of sampling any token. You can use the tf.keras.preprocessing.sequence.make_sampling_table to generate a word-frequency rank based probabilistic sampling table and pass it to the skipgrams function. Inspect the sampling probabilities for a vocab_size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51823b6e-8add-49ab-ba52-242589bed8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae3dbd8-870a-4ced-bf82-9908e2d36976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfdee3-2ef7-4050-a523-68c533e014f3",
   "metadata": {},
   "source": [
    "#### Load Farsi word embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b45b54-0565-468c-87bf-7a788ed36d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"./datasets/fawiki-20181001-pages-articles-multistream 1 - 100000.json\",\n",
    "              \"./datasets/fawiki-20181001-pages-articles-multistream 100001 - 290169.json\",\n",
    "              \"./datasets/fawiki-20181001-pages-articles-multistream 290170 - 580338.json\",\n",
    "              \"./datasets/fawiki-20181001-pages-articles-multistream 580339 - 870507.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9946095-2cc7-4728-ba73-63f587cb5c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Regular expression pattern to match zero-width space Unicode characters\n",
    "zero_width_space_pattern = re.compile(r'\\u200c')\n",
    "\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Remove zero-width space Unicode characters from the line\n",
    "            line = zero_width_space_pattern.sub('', line)\n",
    "            # Parse the line as JSON and append it to the data list\n",
    "            data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "296b669c-5a85-4ef8-ae22-ec544f5c59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fawiki_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09fa8360-b78d-4883-983d-ce93edde52b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Namespace</th>\n",
       "      <th>RedirectList</th>\n",
       "      <th>IsDisambiguationPage</th>\n",
       "      <th>TargetLinksCount</th>\n",
       "      <th>InfoBox</th>\n",
       "      <th>Text</th>\n",
       "      <th>Links</th>\n",
       "      <th>Parents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>صفحهٔ اصلی</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>[صفحهی اصلی, صفحۀ اصلی, صفحه اصلی, صفحهٔ اصلي,...</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>{'Title': '', 'KeysAndValues': []}</td>\n",
       "      <td>پیوند= مقالههای برگزیده – مقاله پیشنهادی هفته\\...</td>\n",
       "      <td>[j F, Y \"(میلادی)\", تقویم میلادی, xij xiF, xiY...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>ویکیپدیا</td>\n",
       "      <td>6</td>\n",
       "      <td>728</td>\n",
       "      <td>0</td>\n",
       "      <td>[ویکی پدیا, ویکیپدیا, دانشنامهٔ ویکیپدیا, دانش...</td>\n",
       "      <td>False</td>\n",
       "      <td>313</td>\n",
       "      <td>{'Title': 'website', 'KeysAndValues': [{'Item1...</td>\n",
       "      <td>ویکیپدیا یک دانشنامه اینترنتی چندزبانه با محتو...</td>\n",
       "      <td>[صفحهٔ اصلی, دانشنامه برخط, فهرست ویکیپدیاها, ...</td>\n",
       "      <td>[ویکیپدیا, اختراعهای آمریکایی, انقلاب علمی, بر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>سالنامه</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>{'Title': '', 'KeysAndValues': []}</td>\n",
       "      <td>سالنامه دفتر یا کتابی است دستنویس یا چاپشده که...</td>\n",
       "      <td>[کتاب, دستنویس, پیشامد, اطلاعات, سالنامه جاودا...</td>\n",
       "      <td>[آکادمی, سالنامهها, کتابها بر پایه نوع]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>اطلاعات</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>191</td>\n",
       "      <td>{'Title': '', 'KeysAndValues': []}</td>\n",
       "      <td>اطلاع یا معلومات (به فارسی افغانستان) یا آگاهی...</td>\n",
       "      <td>[فلسفه, علم, دانش, نظریه اطلاعات, معماری اطلاع...</td>\n",
       "      <td>[اطلاعات, اطلاعات، دانش و عدم قطعیت, علوم اطلا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>محتوای آزاد</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>[محتويات آزاد, محتویات آزاد, محتواي آزاد, محتو...</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>{'Title': '', 'KeysAndValues': []}</td>\n",
       "      <td>این نگاره نمونهای از یک اثر ویرایش شدهاست که ت...</td>\n",
       "      <td>[تعریف آثار فرهنگی آزاد, محتوای باز, اجازهنامه...</td>\n",
       "      <td>[اجازهنامههای متنباز, محتویات آزاد, نرمافزار آ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id        Title  Type  Rank  Namespace  \\\n",
       "0   1   صفحهٔ اصلی     0    38          0   \n",
       "1  42     ویکیپدیا     6   728          0   \n",
       "2  47      سالنامه     0    29          0   \n",
       "3  49      اطلاعات     0   211          0   \n",
       "4  52  محتوای آزاد     0   129          0   \n",
       "\n",
       "                                        RedirectList  IsDisambiguationPage  \\\n",
       "0  [صفحهی اصلی, صفحۀ اصلی, صفحه اصلی, صفحهٔ اصلي,...                 False   \n",
       "1  [ویکی پدیا, ویکیپدیا, دانشنامهٔ ویکیپدیا, دانش...                 False   \n",
       "2                                                 []                 False   \n",
       "3                                                 []                 False   \n",
       "4  [محتويات آزاد, محتویات آزاد, محتواي آزاد, محتو...                 False   \n",
       "\n",
       "   TargetLinksCount                                            InfoBox  \\\n",
       "0                30                 {'Title': '', 'KeysAndValues': []}   \n",
       "1               313  {'Title': 'website', 'KeysAndValues': [{'Item1...   \n",
       "2                15                 {'Title': '', 'KeysAndValues': []}   \n",
       "3               191                 {'Title': '', 'KeysAndValues': []}   \n",
       "4                28                 {'Title': '', 'KeysAndValues': []}   \n",
       "\n",
       "                                                Text  \\\n",
       "0  پیوند= مقالههای برگزیده – مقاله پیشنهادی هفته\\...   \n",
       "1  ویکیپدیا یک دانشنامه اینترنتی چندزبانه با محتو...   \n",
       "2  سالنامه دفتر یا کتابی است دستنویس یا چاپشده که...   \n",
       "3  اطلاع یا معلومات (به فارسی افغانستان) یا آگاهی...   \n",
       "4  این نگاره نمونهای از یک اثر ویرایش شدهاست که ت...   \n",
       "\n",
       "                                               Links  \\\n",
       "0  [j F, Y \"(میلادی)\", تقویم میلادی, xij xiF, xiY...   \n",
       "1  [صفحهٔ اصلی, دانشنامه برخط, فهرست ویکیپدیاها, ...   \n",
       "2  [کتاب, دستنویس, پیشامد, اطلاعات, سالنامه جاودا...   \n",
       "3  [فلسفه, علم, دانش, نظریه اطلاعات, معماری اطلاع...   \n",
       "4  [تعریف آثار فرهنگی آزاد, محتوای باز, اجازهنامه...   \n",
       "\n",
       "                                             Parents  \n",
       "0                                                 []  \n",
       "1  [ویکیپدیا, اختراعهای آمریکایی, انقلاب علمی, بر...  \n",
       "2            [آکادمی, سالنامهها, کتابها بر پایه نوع]  \n",
       "3  [اطلاعات, اطلاعات، دانش و عدم قطعیت, علوم اطلا...  \n",
       "4  [اجازهنامههای متنباز, محتویات آزاد, نرمافزار آ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fawiki_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f04f203f-6803-4b79-9912-6f7bdc84e9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Title', 'Type', 'Rank', 'Namespace', 'RedirectList',\n",
       "       'IsDisambiguationPage', 'TargetLinksCount', 'InfoBox', 'Text', 'Links',\n",
       "       'Parents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fawiki_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8439771c-22e8-45c1-8c7e-f7f362de8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 870507 entries, 0 to 870506\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   Id                    870507 non-null  int64 \n",
      " 1   Title                 870507 non-null  object\n",
      " 2   Type                  870507 non-null  int64 \n",
      " 3   Rank                  870507 non-null  int64 \n",
      " 4   Namespace             870507 non-null  int64 \n",
      " 5   RedirectList          870507 non-null  object\n",
      " 6   IsDisambiguationPage  870507 non-null  bool  \n",
      " 7   TargetLinksCount      870507 non-null  int64 \n",
      " 8   InfoBox               870507 non-null  object\n",
      " 9   Text                  870507 non-null  object\n",
      " 10  Links                 870507 non-null  object\n",
      " 11  Parents               870507 non-null  object\n",
      "dtypes: bool(1), int64(5), object(6)\n",
      "memory usage: 73.9+ MB\n"
     ]
    }
   ],
   "source": [
    "fawiki_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9d69eb-5c16-4fda-a884-2fed8bdafe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پیوند= مقالههای برگزیده – مقاله پیشنهادی هفته\\nدرباره ویکیپدیا\\nدرونمایه\\nاز میان خبرها\\nامروز : ، میلادی برابر هجری خورشیدی و (UTC)\\n-1 day – +1 day یادبودهای – یادبودهای بیشتر…\\nlink= بایگانی – نگارههای برگزیده بیشتر\\nدیگر پروژههای بنیاد ویکیمدیا\\nویکیپدیا در زبانهای دیگر'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fawiki_df.loc[0, \"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97a250-21a4-4865-b770-74b6d7aa39e8",
   "metadata": {},
   "source": [
    "#### Clean up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d014304-9950-4c1e-9f59-fff98f0fc3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_newline_with_space(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "fawiki_df = fawiki_df.map(lambda x: replace_newline_with_space(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1323f6b-c604-4652-b8c3-63f961462e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'۱۴ اسفند - از آغاز سال در گاهشماری ایران ۳۵۰ روز گذشته و به پایان آن ۱۵ روز (در سال عادی) یا ۱۶ روز (در سال کبیسه) ماندهاست. رویدادها ۱۳۵۷ - تاسیس کمیته امداد امام خمینی به دستور سید روحالله خمینی. ۱۳۵۹ - رویدادهای ۱۴ اسفند پس از سخنرانی بنی صدر. زادروزها ۱۰۵۶ - آنتونیو ویوالدی، آهنگساز و ویلنیست اهل ونیز ۱۲۵۴ - لئون-پل فارگ، نویسنده و شاعر اهل فرانسه ۱۳۰۷ - مهدی اخوان ثالث، شاعر معاصر مرگها ۱۳۴۵ - محمد مصدق ، دولتمرد و نخستوزیر ایرانی (زاده ۱۲۶۱). ۱۳۸۸- محمد خوانساری استاد برجسته فلسفه و منطق در دانشگاه تهران، عضو پیوسته فرهنگستان زبان و ادبیات فارسی و یکی از بزرگترین منطق دانان دوران معاصر بود. مناسبتها روز احسان و نیکوکاری . ۵ مارس'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fawiki_df.loc[14, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc8286ee-aa73-4af0-b4fa-4ec8b380ff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    پیوند= مقالههای برگزیده – مقاله پیشنهادی هفته ...\n",
       "1    ویکیپدیا یک دانشنامه اینترنتی چندزبانه با محتو...\n",
       "2    سالنامه دفتر یا کتابی است دستنویس یا چاپشده که...\n",
       "3    اطلاع یا معلومات (به فارسی افغانستان) یا آگاهی...\n",
       "4    این نگاره نمونهای از یک اثر ویرایش شدهاست که ت...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df = fawiki_df['Text']\n",
    "w2v_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49b940e3-1604-4a3e-9e50-c87aac2935b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "text_list = w2v_df.tolist()\n",
    "\n",
    "# Create a TensorFlow dataset from the text list\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2765f684-4b38-4f17-9d10-8c064d59bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "پیوند= مقالههای برگزیده – مقاله پیشنهادی هفته درباره ویکیپدیا درونمایه از میان خبرها امروز : ، میلادی برابر هجری خورشیدی و (UTC) -1 day – +1 day یادبودهای – یادبودهای بیشتر… link= بایگانی – نگارههای برگزیده بیشتر دیگر پروژههای بنیاد ویکیمدیا ویکیپدیا در زبانهای دیگر\n"
     ]
    }
   ],
   "source": [
    "first_element = next(iter(text_ds.take(1)))\n",
    "print(first_element.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb67883-a9bf-4764-a9d6-41736d02a3eb",
   "metadata": {},
   "source": [
    "#### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e527baa-70fe-4ea8-8815-dbb06c5134e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lowercase remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  return tf.strings.regex_replace(input_data, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "878bb2c6-d4e4-45ca-af2d-952362412aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15b40359-f278-4335-a418-ec146afa3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 16:34:04.372421: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Call TextVectorization.adapt on the text dataset to create vocabulary.\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750db82a-0ce8-4eb9-ba85-933c15452f37",
   "metadata": {},
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `TextVectorization.get_vocabulary`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d66f98a7-8369-4af2-be8b-e3466a15a83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'در', 'و', 'به', 'از', 'که', 'است', 'این', 'را', 'با', 'یک', 'سال', 'آن', 'برای', 'شد', 'ایران', 'او', 'بود', 'بر']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee919bba-98d9-49e5-b30a-683274393b88",
   "metadata": {},
   "source": [
    "The `vectorize_layer` can now be used to generate vectors for each element in the `text_ds` (a `tf.data.Dataset`). Apply `Dataset.batch`, `Dataset.prefetch`, `Dataset.map`, and `Dataset.unbatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ccbfaf2-d4ea-4d45-ba99-0a9670418108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2d1dd-e29a-4109-929e-7ff46859e19a",
   "metadata": {},
   "source": [
    "#### Obtain sequences from the dataset\n",
    "\n",
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a word2vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f4a08ef-ae30-4d26-9770-7b51cc112b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 16:36:57.334441: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dd47d-5caa-4a62-9d4b-ab28f59a052b",
   "metadata": {},
   "source": [
    "Let's nspect a few examples from `sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbba7555-0758-49ee-8bda-0f95d5a4aa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1350 1074 1471   74 1338    1 1123  236    1    1] => ['پیوند', 'مقالههای', 'برگزیده', '–', 'مقاله', '[UNK]', 'هفته', 'درباره', '[UNK]', '[UNK]']\n",
      "[   1   11 3636 2691    1   10    1  446    7    6] => ['[UNK]', 'یک', 'دانشنامه', 'اینترنتی', '[UNK]', 'با', '[UNK]', 'آزاد', 'است', 'که']\n",
      "[   1  901   23 1844    7    1   23    1    6    1] => ['[UNK]', 'دفتر', 'یا', 'کتابی', 'است', '[UNK]', 'یا', '[UNK]', 'که', '[UNK]']\n",
      "[3046   23    1    4  296  638   23    1    2    1] => ['اطلاع', 'یا', '[UNK]', 'به', 'فارسی', 'افغانستان', 'یا', '[UNK]', 'در', '[UNK]']\n",
      "[   8    1 3406    5   11   99 3238   31    6    1] => ['این', '[UNK]', 'نمونهای', 'از', 'یک', 'اثر', 'ویرایش', 'شدهاست', 'که', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2d29d-c0cc-4b26-b299-5b7d2dd79948",
   "metadata": {},
   "source": [
    "#### Generate training examples from sequences\n",
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data` function defined earlier to generate training examples for the word2vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be the same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0260dc68-d455-484c-a59e-3a0403920473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 870507/870507 [04:27<00:00, 3254.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (2174972,)\n",
      "contexts.shape: (2174972, 5)\n",
      "labels.shape: (2174972, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f754f73-fde6-4a79-ad41-74bb2517cf2c",
   "metadata": {},
   "source": [
    "#### Configure the dataset for performance\n",
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your word2vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "508dfa3e-6a99-46c1-8ba0-0d549153489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55f4d8cc-c27c-4897-8a94-ee0aea72309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd7aa4-d134-42c5-9b72-49d86bf9f261",
   "metadata": {},
   "source": [
    "#### Model and training\n",
    "\n",
    "The word2vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product multiplication between the embeddings of target and context words to obtain predictions for labels and compute the loss function against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154ea11-54bb-416d-a35e-dd5fc61a79fe",
   "metadata": {},
   "source": [
    "#### Subclassed word2vec model\n",
    "Use the Keras Subclassing API to define your word2vec model with the following layers:\n",
    "\n",
    "- `target_embedding`: A `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "- `context_embedding`: Another `tf.keras.layers.Embedding` layer, which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "- `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "- `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.<br>\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75001541-fdda-4bb8-9e17-c5664dc20465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e6e98-b9ce-4927-86aa-d305759f72d8",
   "metadata": {},
   "source": [
    "#### Define loss function and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6cfed05-91c2-4361-b150-7a48e92c5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54516bd5-edad-4bdc-ad49-fc28769c0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77c3730e-a097-433b-87e2-bf29b32c8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a469328a-1d0e-465a-ba9f-768cf528384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.6650 - loss: 0.9327\n",
      "Epoch 2/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7750 - loss: 0.6340\n",
      "Epoch 3/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8025 - loss: 0.5649\n",
      "Epoch 4/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.5255\n",
      "Epoch 5/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.4968\n",
      "Epoch 6/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8378 - loss: 0.4734\n",
      "Epoch 7/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8455 - loss: 0.4534\n",
      "Epoch 8/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8524 - loss: 0.4357\n",
      "Epoch 9/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8586 - loss: 0.4200\n",
      "Epoch 10/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.4059\n",
      "Epoch 11/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8687 - loss: 0.3934\n",
      "Epoch 12/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8730 - loss: 0.3823\n",
      "Epoch 13/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8765 - loss: 0.3724\n",
      "Epoch 14/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8798 - loss: 0.3637\n",
      "Epoch 15/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8826 - loss: 0.3559\n",
      "Epoch 16/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8850 - loss: 0.3491\n",
      "Epoch 17/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8871 - loss: 0.3430\n",
      "Epoch 18/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8889 - loss: 0.3376\n",
      "Epoch 19/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8904 - loss: 0.3329\n",
      "Epoch 20/20\n",
      "\u001b[1m2123/2123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8917 - loss: 0.3286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x61622f690>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1173aa4-a3e8-47ec-a276-05af0f8070a8",
   "metadata": {},
   "source": [
    "#### Embedding lookup and analysis\n",
    "Obtain the weights from the model using `Model.get_layer` and `Layer.get_weights`. The `TextVectorization.get_vocabulary` function provides the vocabulary to build a metadata file with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54e6498c-7d3a-48fb-aa03-d8dc047da244",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc4d9d1d-ac0b-4a10-a80e-d9bab7d4052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the vectors and metadata files\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9e857-1b2f-4031-b08c-b5b94fd3dd4f",
   "metadata": {},
   "source": [
    "Download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the Embedding Projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd35b826-6fc2-4f0d-9e27-e39716e97793",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73da1f-1153-4721-ac6d-5a915e81f7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
